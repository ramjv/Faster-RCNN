{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FasterRCNN_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN4UJQ/+ECHrOkY/EM/NRFG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramjv/Faster-RCNN/blob/master/FasterRCNN_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GfczHZR0VmQK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import absolute_import\n",
        "import random\n",
        "import pprint\n",
        "import sys\n",
        "import time\n",
        "import numpy as np\n",
        "from optparse import OptionParser\n",
        "import pickle\n",
        "import math\n",
        "import cv2\n",
        "import copy\n",
        "from matplotlib import pyplot as plt\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.optimizers import Adam, SGD, RMSprop\n",
        "from tensorflow.keras.layers import Flatten, Dense, Input, Conv2D, MaxPooling2D, Dropout\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, GlobalMaxPooling2D, TimeDistributed\n",
        "#from tensorflow.keras.engine.topology import get_source_inputs\n",
        "#from tensorflow.keras.utils import layer_utils\n",
        "#from tensorflow.keras.utils.data_utils import get_file\n",
        "#from tensorflow.keras.objectives import categorical_crossentropy\n",
        "\n",
        "from tensorflow.keras.models import Model\n",
        "#from tensorflow.keras.utils import generic_utils\n",
        "#from tensorflow.keras.engine import Layer, InputSpec\n",
        "from tensorflow.keras import initializers, regularizers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvGeC5juInt4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config:\n",
        "\n",
        "  def __init__(self):\n",
        "\n",
        "    self.name = 'ram'\n",
        "    # Print the process or not\n",
        "    self.verbose = True\n",
        "\n",
        "    # Name of base network\n",
        "    self.network = 'vgg'\n",
        "\n",
        "    # Setting for data augmentation\n",
        "    self.use_horizontal_flips = False\n",
        "    self.use_vertical_flips = False\n",
        "    self.rot_90 = False\n",
        "\n",
        "    # Anchor box scales\n",
        "    # Note that if im_size is smaller, anchor_box_scales should be scaled\n",
        "    # Original anchor_box_scales in the paper is [128, 256, 512]\n",
        "\n",
        "    self.anchor_box_scales = [64, 128, 256] \n",
        "\n",
        "    # Anchor box ratios\n",
        "    self.anchor_box_ratios = [[1, 1], [1./math.sqrt(2), 2./math.sqrt(2)], [2./math.sqrt(2), 1./math.sqrt(2)]]\n",
        "\n",
        "    # Size to resize the smallest side of the image\n",
        "    # Original setting in paper is 600. Set to 300 in here to save training time\n",
        "    self.im_size = 300\n",
        "\n",
        "    # image channel-wise mean to subtract\n",
        "    self.img_channel_mean = [103.939, 116.779, 123.68]\n",
        "    self.img_scaling_factor = 1.0\n",
        "\n",
        "    # number of ROIs at once\n",
        "    self.num_rois = 4\n",
        "\n",
        "    # stride at the RPN (this depends on the network configuration)\n",
        "    self.rpn_stride = 16\n",
        "\n",
        "    self.balanced_classes = False\n",
        "\n",
        "    # scaling the stdev\n",
        "    self.std_scaling = 4.0\n",
        "    self.classifier_regr_std = [8.0, 8.0, 4.0, 4.0]\n",
        "\n",
        "    # overlaps for RPN\n",
        "    self.rpn_min_overlap = 0.3\n",
        "    self.rpn_max_overlap = 0.7\n",
        "\n",
        "    # overlaps for classifier ROIs\n",
        "    self.classifier_min_overlap = 0.1\n",
        "    self.classifier_max_overlap = 0.5\n",
        "\n",
        "    # placeholder for the class mapping, automatically generated by the parser\n",
        "    self.class_mapping = None\n",
        "\n",
        "    self.model_path = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJy_z8GTKloq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(input_path):\n",
        "\t\"\"\"Parse the data from annotation file\n",
        "\t\n",
        "\tArgs:\n",
        "\t\tinput_path: annotation file path\n",
        "\n",
        "\tReturns:\n",
        "\t\tall_data: list(filepath, width, height, list(bboxes))\n",
        "\t\tclasses_count: dict{key:class_name, value:count_num} \n",
        "\t\t\te.g. {'Car': 2383, 'Mobile phone': 1108, 'Person': 3745}\n",
        "\t\tclass_mapping: dict{key:class_name, value: idx}\n",
        "\t\t\te.g. {'Car': 0, 'Mobile phone': 1, 'Person': 2}\n",
        "\t\"\"\"\n",
        "\t\n",
        "\timgpath = '/content/JPEGImages/'\n",
        "\tfound_bg = False\n",
        "\tall_imgs = {}\n",
        "\n",
        "\tclasses_count = {}\n",
        "\n",
        "\tclass_mapping = {}\n",
        "\n",
        "\tvisualise = True\n",
        "\n",
        "\ti = 1\n",
        "\t\n",
        "\twith open(input_path,'r') as f:\n",
        "    \n",
        "\t\tprint('Parsing annotation files')\n",
        "\n",
        "\t\tfor line in f:\n",
        "\n",
        "\t\t\t# Print process\n",
        "\t\t\tsys.stdout.write('\\r'+'idx=' + str(i))\n",
        "\t\t\ti += 1\n",
        "\n",
        "\t\t\tline_split = line.strip().split(',')\n",
        "\n",
        "\t\t\t# Make sure the info saved in annotation file matching the format (path_filename, x1, y1, x2, y2, class_name)\n",
        "\t\t\t# Note:\n",
        "\t\t\t#\tOne path_filename might has several classes (class_name)\n",
        "\t\t\t#\tx1, y1, x2, y2 are the pixel value of the origial image, not the ratio value\n",
        "\t\t\t#\t(x1, y1) top left coordinates; (x2, y2) bottom right coordinates\n",
        "\t\t\t#   x1,y1-------------------\n",
        "\t\t\t#\t|\t\t\t\t\t\t|\n",
        "\t\t\t#\t|\t\t\t\t\t\t|\n",
        "\t\t\t#\t|\t\t\t\t\t\t|\n",
        "\t\t\t#\t|\t\t\t\t\t\t|\n",
        "\t\t\t#\t---------------------x2,y2\n",
        "\n",
        "\t\t\t(filename,x1,y1,x2,y2,class_name) = line_split\n",
        "      \n",
        "      \n",
        "\t\t\tif class_name not in classes_count:\n",
        "\t\t\t\tclasses_count[class_name] = 1\n",
        "\t\t\telse:\n",
        "\t\t\t\tclasses_count[class_name] += 1\n",
        "\n",
        "\t\t\tif class_name not in class_mapping:\n",
        "\t\t\t\tif class_name == 'bg' and found_bg == False:\n",
        "\t\t\t\t\tprint('Found class name with special name bg. Will be treated as a background region (this is usually for hard negative mining).')\n",
        "\t\t\t\t\tfound_bg = True\n",
        "\t\t\t\tclass_mapping[class_name] = len(class_mapping)\n",
        "\n",
        "\t\t\tif filename not in all_imgs:\n",
        "\t\t\t\tall_imgs[filename] = {}\n",
        "\t\t\t\t\n",
        "\t\t\t\timg = cv2.imread(imgpath+filename)\n",
        "\t\t\t\t(rows,cols) = img.shape[:2]\n",
        "\t\t\t\tall_imgs[filename]['filepath'] = imgpath+filename\n",
        "\t\t\t\tall_imgs[filename]['width'] = cols\n",
        "\t\t\t\tall_imgs[filename]['height'] = rows\n",
        "\t\t\t\tall_imgs[filename]['bboxes'] = []\n",
        "\t\t\t\t# if np.random.randint(0,6) > 0:\n",
        "\t\t\t\t# \tall_imgs[filename]['imageset'] = 'trainval'\n",
        "\t\t\t\t# else:\n",
        "\t\t\t\t# \tall_imgs[filename]['imageset'] = 'test'\n",
        "\n",
        "\t\t\tall_imgs[filename]['bboxes'].append({'class': class_name, 'x1': int(x1), 'x2': int(x2), 'y1': int(y1), 'y2': int(y2)})\n",
        "\n",
        "\n",
        "\t\tall_data = []\n",
        "\t\tfor key in all_imgs:\n",
        "\t\t\tall_data.append(all_imgs[key])\n",
        "\t\t\n",
        "\t\t# make sure the bg class is last in the list\n",
        "\t\tif found_bg:\n",
        "\t\t\tif class_mapping['bg'] != len(class_mapping) - 1:\n",
        "\t\t\t\tkey_to_switch = [key for key in class_mapping.keys() if class_mapping[key] == len(class_mapping)-1][0]\n",
        "\t\t\t\tval_to_switch = class_mapping['bg']\n",
        "\t\t\t\tclass_mapping['bg'] = len(class_mapping) - 1\n",
        "\t\t\t\tclass_mapping[key_to_switch] = val_to_switch\n",
        "\t\t\n",
        "\t\treturn all_data, classes_count, class_mapping\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qypGZp5r9GBt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RoiPoolingConv(Layer):\n",
        "    '''ROI pooling layer for 2D inputs.\n",
        "    See Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition,\n",
        "    K. He, X. Zhang, S. Ren, J. Sun\n",
        "    # Arguments\n",
        "        pool_size: int\n",
        "            Size of pooling region to use. pool_size = 7 will result in a 7x7 region.\n",
        "        num_rois: number of regions of interest to be used\n",
        "    # Input shape\n",
        "        list of two 4D tensors [X_img,X_roi] with shape:\n",
        "        X_img:\n",
        "        `(1, rows, cols, channels)`\n",
        "        X_roi:\n",
        "        `(1,num_rois,4)` list of rois, with ordering (x,y,w,h)\n",
        "    # Output shape\n",
        "        3D tensor with shape:\n",
        "        `(1, num_rois, channels, pool_size, pool_size)`\n",
        "    '''\n",
        "    def __init__(self, pool_size, num_rois, **kwargs):\n",
        "\n",
        "        self.dim_ordering = K.image_dim_ordering()\n",
        "        self.pool_size = pool_size\n",
        "        self.num_rois = num_rois\n",
        "\n",
        "        super(RoiPoolingConv, self).__init__(**kwargs)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.nb_channels = input_shape[0][3]   \n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return None, self.num_rois, self.pool_size, self.pool_size, self.nb_channels\n",
        "\n",
        "    def call(self, x, mask=None):\n",
        "\n",
        "        assert(len(x) == 2)\n",
        "\n",
        "        # x[0] is image with shape (rows, cols, channels)\n",
        "        img = x[0]\n",
        "\n",
        "        # x[1] is roi with shape (num_rois,4) with ordering (x,y,w,h)\n",
        "        rois = x[1]\n",
        "\n",
        "        input_shape = K.shape(img)\n",
        "\n",
        "        outputs = []\n",
        "\n",
        "        for roi_idx in range(self.num_rois):\n",
        "\n",
        "            x = rois[0, roi_idx, 0]\n",
        "            y = rois[0, roi_idx, 1]\n",
        "            w = rois[0, roi_idx, 2]\n",
        "            h = rois[0, roi_idx, 3]\n",
        "\n",
        "            x = K.cast(x, 'int32')\n",
        "            y = K.cast(y, 'int32')\n",
        "            w = K.cast(w, 'int32')\n",
        "            h = K.cast(h, 'int32')\n",
        "\n",
        "            # Resized roi of the image to pooling size (7x7)\n",
        "            rs = tf.image.resize_images(img[:, y:y+h, x:x+w, :], (self.pool_size, self.pool_size))\n",
        "            outputs.append(rs)\n",
        "                \n",
        "\n",
        "        final_output = K.concatenate(outputs, axis=0)\n",
        "\n",
        "        # Reshape to (1, num_rois, pool_size, pool_size, nb_channels)\n",
        "        # Might be (1, 4, 7, 7, 3)\n",
        "        final_output = K.reshape(final_output, (1, self.num_rois, self.pool_size, self.pool_size, self.nb_channels))\n",
        "\n",
        "        # permute_dimensions is similar to transpose\n",
        "        final_output = K.permute_dimensions(final_output, (0, 1, 2, 3, 4))\n",
        "\n",
        "        return final_output\n",
        "    \n",
        "    \n",
        "    def get_config(self):\n",
        "        config = {'pool_size': self.pool_size,\n",
        "                  'num_rois': self.num_rois}\n",
        "        base_config = super(RoiPoolingConv, self).get_config()\n",
        "        return dict(list(base_config.items()) + list(config.items()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}